# Image Captioning (IC) dataset
The dataset has been designed for the task of recognizing whether a caption is a good (positive) or a bad (negative) caption for an image. The dataset has been generated by exploiting both the Foil [1] and the MSCOCO [2] datasets. We expect Foil to provide difficult negative captions, and MSCOCO to provide easy negative captions. In particular, the dataset generation procedure is described as follows. Separately, for each image contained in the training set/test set of Foil, we sample:
- two positive captions from MSCOCO;
- one negative caption from Foil;
- one negative caption from MSCOCO such that it corresponds to an image with a number of categories in common with the actual image which is less or equal than two.

We consider Foil as a starting point because Foil is a subset of MSCOCO, so if we take images from Foil we can be sure that we can find negative captions both from Foil and from MSCOCO for each image. The script implementing the dataset generation procedure has been parametrized to allow to sample the desired number of positive/negative captions for each image, while maintaining the dataset balanced.

The script implementing the dataset generation procedure has been published [here](https://github.com/hoavt-54/nli-images/blob/master/models/build_ic_dataset.py).

The dataset generated using the described procedure is contained in the folder [IC](https://github.com/hoavt-54/nli-images/tree/master/datasets/IC). The test set has been split into test and development sets with a ratio of 50/50, while maintaining each split balanced.

The statistics of the dataset are reported as follows:

@ TODO

[1] Shekhar, Ravi, et al. "FOIL it! Find One mismatch between Image and Language caption." Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2017.

[2] Lin, Tsung-Yi, et al. "Microsoft coco: Common objects in context." European conference on computer vision. Springer, Cham, 2014.
